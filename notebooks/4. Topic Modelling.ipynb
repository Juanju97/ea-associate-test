{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Topic Modelling\n",
    "### Juan Julián Cea Morán\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Electronic-Arts-Logo.svg/1200px-Electronic-Arts-Logo.svg.png\" width=100px>\n",
    "\n",
    "---\n",
    "Topic Modelling is a classic task in NLP field consisting on discovering abstract topics hidden in the data corpus. This is usefull for example, to understand our data or when we want to make unsupervised classification tasks. \n",
    "\n",
    "In this case, due to the nature of the data you are working with, Topic Modelling can be performed in different ways. The first approach would be to carry out a study of topics for each language present in each of the 4 contexts. The other option consists of carrying out Topic Modelling for each context without taking into account the different languages.\n",
    "\n",
    "In this case, the second option is chosen, although certain key points must be taken into account for its correct implementation. The main problem to solve is to be able to represent words from different languages in the same vector space so that they are related to each other independently of the language but according to the context. For this, we are be using multilingual word embeddings.\n",
    "\n",
    "With regarding the model, there are some options: LDA, NMF, LSI, etc. We are going to start with LDA since is the most used.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "As we saw in classification notebook, the first step is to prepare the data. The main diference is that in this case, we don't need to partition our dataset since this is not the same kind problem.\n",
    "\n",
    "First of all, we have to import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "preproc_df = pickle.load(open(\"../data/preproc_df.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preprocessed</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[read, book, town, everyone, uses, order, phar...</td>\n",
       "      <td>APR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[recipes, appreciated, family, small, large, r...</td>\n",
       "      <td>APR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[say, ease, author, even, made, effort, meet, ...</td>\n",
       "      <td>APR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[milady, found, good, vein, anita, blake, base...</td>\n",
       "      <td>APR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[somewhere, greece, gentlemen, decided, visit,...</td>\n",
       "      <td>APR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Preprocessed Category\n",
       "0  [read, book, town, everyone, uses, order, phar...      APR\n",
       "1  [recipes, appreciated, family, small, large, r...      APR\n",
       "2  [say, ease, author, even, made, effort, meet, ...      APR\n",
       "3  [milady, found, good, vein, anita, blake, base...      APR\n",
       "4  [somewhere, greece, gentlemen, decided, visit,...      APR"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I said before, I'm going to perform Topic Modelling for the four different context in the data, so the results show abdstrac topics whithin those categories. This means that is necessary to split data by context/category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "apr_data = [' '.join(text) for text in preproc_df.loc[(preproc_df['Category'] == 'APR')]['Preprocessed']]\n",
    "conference_data = [' '.join(text) for text in preproc_df.loc[(preproc_df['Category'] == 'Conference_papers')]['Preprocessed']]\n",
    "pan_data = [' '.join(text) for text in preproc_df.loc[(preproc_df['Category'] == 'PAN11')]['Preprocessed']]\n",
    "wiki_data = [' '.join(text) for text in preproc_df.loc[(preproc_df['Category'] == 'Wikipedia')]['Preprocessed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we can use bigrams or trigrams (n-grams) as well as lemmatization or steeming at the preprocessing step. As I said in the 2. Preprocessing notebook conclusions, this is going to be a future work feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Different Approaches\n",
    "* **Bag of Words + LDA:** When working with LDA, the usual vectorization model is Bag of Words. This model builds a dictionary with all the different words found in the corpus. Then, builds sparse vectors for each document with the same dimensionality as number of words in the dictionary. For each one of those vectors representing each document, a 1 is set on the position of a certain word in the vector if that document contains that word. This kind of vectorization carries an extreamly high dimensionality representation of the documents. Another problem of this approach is that there is no information about how the different terms in the documents are related, like in a classic one-hot encoding vectorization. However, using algorithms such LDA or NMF to reduce dimensionality of BoW models is widely used in the literature, and very efective when dealing with big amounts of data.\n",
    "\n",
    "* **Word embeddings:** Other novel option is word. This model address the problem of capturing semantic dependencies between words in the corpus so the spatial representation makes sense with the real world. For example, in a classic one-hot encodding the word *cat* would be at the same distance from *dog* tan from *pencil*. The main idea behind Word Embeddings is that when you represent those words, *cat* and *dog* are closer to each other than they are to *pencil*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Apply LDA\n",
    "Once we have the data, it's time to apply the model. As I said, the first model to be tested is LDA since it is widely used for this task. LDA considers each document in the corpus as a mix of hidden topics and each one of this topic, as a mix of keywords.\n",
    "\n",
    "Let's start making Topic Modelling for Wikipedia docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Conclusions of Classification task**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
