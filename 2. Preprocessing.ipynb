{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing\n",
    "#### Juan Julián Cea Morán\n",
    "---\n",
    "As we saw in the EDA section, we are working with text in 3 possible languages: english, spanish and french. We have no indication that the final system can work with more languages, so we will only focus on those three.\n",
    "\n",
    "This means that we are working with a mulitlingual dataset. To solve this task it will be necessary to apply textual pre-processing adapted to each of the different languages present in the dataset. \n",
    "\n",
    "There are several ways to achieve this requierement: for example, it could be done by building regular expressions so all exceptions founded in texts could be captured. Another option would be breaking the data apart by language and use specifict tools for each language.\n",
    "\n",
    "In this case we are using the second approach, so we have to identify th language of each text and then apply specifict transformation functions.\n",
    "\n",
    "The first step is to take pandas dataframe from EDA section and use the 'text' column to preprocess each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "docs_df = pickle.load(open(\"data/docs_df.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Text</th>\n",
       "      <th>Lang</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apr-book-0-en.txt</td>\n",
       "      <td>i read this book because in my town, everyone ...</td>\n",
       "      <td>en</td>\n",
       "      <td>APR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apr-book-1-en.txt</td>\n",
       "      <td>recipes appreciated by the family (small and l...</td>\n",
       "      <td>en</td>\n",
       "      <td>APR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apr-book-10-en.txt</td>\n",
       "      <td>i say no to ease ..... and not to the author w...</td>\n",
       "      <td>en</td>\n",
       "      <td>APR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apr-book-100-en.txt</td>\n",
       "      <td>milady has found a good vein: anita blake. bas...</td>\n",
       "      <td>en</td>\n",
       "      <td>APR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apr-book-1000-en.txt</td>\n",
       "      <td>460 bc, somewhere in greece: \"gentlemen, i dec...</td>\n",
       "      <td>en</td>\n",
       "      <td>APR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Name                                               Text  \\\n",
       "0     apr-book-0-en.txt  i read this book because in my town, everyone ...   \n",
       "1     apr-book-1-en.txt  recipes appreciated by the family (small and l...   \n",
       "2    apr-book-10-en.txt  i say no to ease ..... and not to the author w...   \n",
       "3   apr-book-100-en.txt  milady has found a good vein: anita blake. bas...   \n",
       "4  apr-book-1000-en.txt  460 bc, somewhere in greece: \"gentlemen, i dec...   \n",
       "\n",
       "  Lang Category  \n",
       "0   en      APR  \n",
       "1   en      APR  \n",
       "2   en      APR  \n",
       "3   en      APR  \n",
       "4   en      APR  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to work with 'Text' column. Then, dump the result as a new column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i read this book because in my town, everyone ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recipes appreciated by the family (small and l...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i say no to ease ..... and not to the author w...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>milady has found a good vein: anita blake. bas...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>460 bc, somewhere in greece: \"gentlemen, i dec...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Lang\n",
       "0  i read this book because in my town, everyone ...   en\n",
       "1  recipes appreciated by the family (small and l...   en\n",
       "2  i say no to ease ..... and not to the author w...   en\n",
       "3  milady has found a good vein: anita blake. bas...   en\n",
       "4  460 bc, somewhere in greece: \"gentlemen, i dec...   en"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_df = docs_df[['Text', 'Lang']]\n",
    "lang_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language identification\n",
    "There are some python libraries that address this issue. For example: \n",
    "* LangID: Developped by Lui Marco, is a standalone Language Identification (LangID) tool bassed on previous researchs by the authors. https://github.com/saffsd/langid.py\n",
    "* langdetect: Port of Nakatani Shuyo's language-detection library (version from 03/03/2014) to Python. https://github.com/Mimino666/langdetect\n",
    "* TextBlob: A popular Text Processing package for Python. It includes a module for language detection based on a Google API. Requires Internet conection. https://textblob.readthedocs.io/en/dev/\n",
    "\n",
    "Let's compare them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take text in the 3 languages\n",
    "en = lang_df.loc[(lang_df['Lang'] == 'en')][0:100]\n",
    "es = lang_df.loc[(lang_df['Lang'] == 'es')][0:100]\n",
    "fr = lang_df.loc[(lang_df['Lang'] == 'fr')][0:100]\n",
    "\n",
    "sample_texts = pd.concat([en, es, fr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langid\n",
    "\n",
    "_langid = [l [0] for l in sample_texts['Text'].apply(langid.classify)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "_langdetect = [l for l in sample_texts['Text'].apply(langdetect.detect)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "_textblob = [TextBlob(l).detect_language() for l in sample_texts['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangID Error: 0\n",
      "LangDetect Error: 1\n",
      "TextBlob Error: 0\n"
     ]
    }
   ],
   "source": [
    "langid_error =  0\n",
    "langdetect_error = 0\n",
    "textblob_error = 0\n",
    "\n",
    "sample_texts_lang = [s for s in sample_texts['Lang']]\n",
    "\n",
    "for i in range(0, len(sample_texts_lang)):\n",
    "    if _langid[i] != sample_texts_lang[i]:\n",
    "        langid_error += 1\n",
    "    if _langdetect[i] != sample_texts_lang[i]:\n",
    "        langdetect_error += 1\n",
    "    if _textblob[i] != sample_texts_lang[i]:\n",
    "        textblob_error += 1\n",
    "        \n",
    "print('LangID Error:', langid_error)\n",
    "print('LangDetect Error:', langdetect_error)\n",
    "print('TextBlob Error:', textblob_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A we can see, we get similar results using any of this packages. We are going to use the first one, langID. Let's try it with the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_langid = [l [0] for l in lang_df['Text'].apply(langid.classify)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1656\n"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "\n",
    "full_langs = [s for s in lang_df['Lang']]\n",
    "\n",
    "for i in range(0, len(full_langs)):\n",
    "    if full_langid[i] != full_langs[i]:\n",
    "        error += 1\n",
    "\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, this approach can perform well with a little subsample of the original dataset, but it return 1656 miss classifications when using the entire dataset. Due to that, we are going to use a preprocessing function capable of perform well in every language.\n",
    "\n",
    "---\n",
    "### Cleaning and tokenize\n",
    "We have to build a function that takes every text sample and prepares it for being analyzed by our ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import unidecode\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stopwords = stopwords.words(\"french\") + stopwords.words(\"english\") + stopwords.words(\"spanish\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = unidecode.unidecode(text.strip().lower())\n",
    "    \n",
    "    url_regex = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    text = url_regex.sub(' ', text)\n",
    "    \n",
    "    html_pattern = re.compile('<.?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    text =  html_pattern.sub(' ', text)\n",
    "    \n",
    "    chars_regex = re.compile('[^a-zA-Z0-9 -]')\n",
    "    text = chars_regex.sub(' ', text)\n",
    "\n",
    "    number_regex = re.compile('[\\d]+')\n",
    "    text = number_regex.sub(' ', text)\n",
    "    \n",
    "    text = ' '.join(text.split('\\''))\n",
    "    \n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    text = [word for word in text if word not in stopwords and len(word)>2]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "preprocessed = list()\n",
    "for text in docs_df['Text']:\n",
    "    preprocessed.append(preprocess(text))\n",
    "    \n",
    "docs_df = docs_df.drop(columns=['Preprocessed'])\n",
    "docs_df.insert(2, 'Preprocessed', preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Text</th>\n",
       "      <th>Preprocessed</th>\n",
       "      <th>Lang</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apr-book-0-en.txt</td>\n",
       "      <td>i read this book because in my town, everyone ...</td>\n",
       "      <td>[read, book, town, everyone, uses, order, phar...</td>\n",
       "      <td>en</td>\n",
       "      <td>APR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apr-book-1-en.txt</td>\n",
       "      <td>recipes appreciated by the family (small and l...</td>\n",
       "      <td>[recipes, appreciated, family, small, large, r...</td>\n",
       "      <td>en</td>\n",
       "      <td>APR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apr-book-10-en.txt</td>\n",
       "      <td>i say no to ease ..... and not to the author w...</td>\n",
       "      <td>[say, ease, author, even, made, effort, meet, ...</td>\n",
       "      <td>en</td>\n",
       "      <td>APR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apr-book-100-en.txt</td>\n",
       "      <td>milady has found a good vein: anita blake. bas...</td>\n",
       "      <td>[milady, found, good, vein, anita, blake, base...</td>\n",
       "      <td>en</td>\n",
       "      <td>APR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apr-book-1000-en.txt</td>\n",
       "      <td>460 bc, somewhere in greece: \"gentlemen, i dec...</td>\n",
       "      <td>[somewhere, greece, gentlemen, decided, visit,...</td>\n",
       "      <td>en</td>\n",
       "      <td>APR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Name                                               Text  \\\n",
       "0     apr-book-0-en.txt  i read this book because in my town, everyone ...   \n",
       "1     apr-book-1-en.txt  recipes appreciated by the family (small and l...   \n",
       "2    apr-book-10-en.txt  i say no to ease ..... and not to the author w...   \n",
       "3   apr-book-100-en.txt  milady has found a good vein: anita blake. bas...   \n",
       "4  apr-book-1000-en.txt  460 bc, somewhere in greece: \"gentlemen, i dec...   \n",
       "\n",
       "                                        Preprocessed Lang Category  \n",
       "0  [read, book, town, everyone, uses, order, phar...   en      APR  \n",
       "1  [recipes, appreciated, family, small, large, r...   en      APR  \n",
       "2  [say, ease, author, even, made, effort, meet, ...   en      APR  \n",
       "3  [milady, found, good, vein, anita, blake, base...   en      APR  \n",
       "4  [somewhere, greece, gentlemen, decided, visit,...   en      APR  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Explore results\n",
    "Now that we have our texts preprocessed, it's time to explore the data and check if there are irrelevant terms not considered as stop words. This process is important because we want our model to perform well by only \"paying attention\" to relevant terms in the corpus.\n",
    "\n",
    "Let's generate TF-IDF of the results extract some insights. For this test, lets separate texts by language. For the moment we don't need to worry about categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_texts = [' '.join(text) for text in docs_df.loc[(docs_df['Lang'] == 'en')]['Preprocessed']]\n",
    "es_texts = [' '.join(text) for text in docs_df.loc[(docs_df['Lang'] == 'es')]['Preprocessed']]\n",
    "fr_texts = [' '.join(text) for text in docs_df.loc[(docs_df['Lang'] == 'fr')]['Preprocessed']]\n",
    "\n",
    "# total = [' '.join(text) for text in docs_df.loc[(docs_df['Lang'] == 'es')]['Preprocessed'][0:10000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 23.2 GiB for an array with shape (9724, 319558) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-248-236390b947bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0men_tfidf_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0men_tfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0men_feature_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_tfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0men_tfidf_sorting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_tfidf_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0men_top_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtfidf_sorting\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 23.2 GiB for an array with shape (9724, 319558) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "# Top keywords.\n",
    "n = 5\n",
    "\n",
    "# English\n",
    "en_tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "\n",
    "en_tfidf = en_tfidf_vectorizer.fit_transform(en_texts)\n",
    "en_feature_array = np.array(en_tfidf_vectorizer.get_feature_names())\n",
    "en_tfidf_sorting = np.argsort(en_tfidf_t.toarray()).flatten()[::-1]\n",
    "en_top_n = feature_array[tfidf_sorting][:n]\n",
    "\n",
    "\n",
    "# en_tfidf = en_tfidf_vectorizer.fit(en_texts)\n",
    "# en_tfidf = en_tfidf_vectorizer.fit_transform(en_texts)\n",
    "# en_feature_array = np.array(en_tfidf_vectorizer.get_feature_names())\n",
    "# en_tfidf_sorting = np.argsort(en_tfidf.toarray()).flatten()[::-1]\n",
    "# en_top_n = feature_array[tfidf_sorting][:n]\n",
    "\n",
    "# # Spanish\n",
    "# es_tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "# es_tfidf = es_tfidf_vectorizer.fit_transform(es_texts)\n",
    "# es_feature_array = np.array(es_tfidf_vectorizer.get_feature_names())\n",
    "# es_tfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]\n",
    "# es_top_n = feature_array[tfidf_sorting][:n]\n",
    "\n",
    "# # French\n",
    "# fr_tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "# fr_tfidf = fr_tfidf_vectorizer.fit_transform(fr_texts)\n",
    "# fr_feature_array = np.array(fr_tfidf_vectorizer.get_feature_names())\n",
    "# fr_tfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]\n",
    "# fr_top_n = feature_array[tfidf_sorting][:n]\n",
    "\n",
    "\n",
    "# fr_tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "# fr_tfidf = fr_tfidf_vectorizer.fit_transform(total)\n",
    "# fr_tfidf_feature_names = fr_tfidf_vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now we have to export data so we can work with it in the next sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_df = docs_df.drop(columns=[\"Name\", \"Text\", \"Lang\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "output = open('data/preproc_df.pkl', 'wb')\n",
    "pickle.dump (preproc_df, output)\n",
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
